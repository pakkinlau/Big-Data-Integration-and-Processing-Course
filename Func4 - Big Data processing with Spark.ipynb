{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How spark works?\n",
    "\n",
    "Q: how spark can process data bigger than memory? it write something on the disk?\n",
    "\n",
    "A: Spark can process data that is larger than the memory by employing a combination of in-memory processing and disk storage. \n",
    "\n",
    "1. RDDs - immutable distributed collection of objects that can be processed in parallel. RDDs can be stored in memory or on disk across a cluster, and Spark automatically manages their partitioning.\n",
    "\n",
    "2. Lazy evalvation\n",
    "-  Spark constructs a Directed Acyclic Graph (DAG) of transformations, and the actual computation is triggered only when an action (like collect, count, saveAsTextFile, etc.) is called. \n",
    "\n",
    "3. Persistence (Caching):\n",
    "- Users can indicate which RDDs they will reuse and choose a storage strategy for them (memory only, disk only, or a combination). \n",
    "\n",
    "4. Shuffle operation \n",
    "- During operations like groupBy or reduceByKey, data needs to be shuffled across different nodes.\n",
    "\n",
    "6. Partitioning\n",
    "- Spark automatically partitions RDDs across the cluster and executes operations on them in parallel. But you can still manually define them. \n",
    "\n",
    "7. Backpressure mechanism\n",
    "- the backpressure mechanism dynamically adjusts the rate of data ingestion so that the system does not get overwhelmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
